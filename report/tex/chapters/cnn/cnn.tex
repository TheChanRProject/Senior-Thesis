\doublespacing
\setlength{\parindent}{1cm}

To understand Convolutional Neural Networks, we have to understand the following foundational ideas:

\begin{itemize}
  \item Convolution
  \item Pooling
  \item Jargon: padding, stride, filter, etc
\end{itemize}

In computer vision, we have used diverse techniques in the past on images to do object detection and image classification. One major problem with computer vision problems is that the input data can get really big. Suppose an image is of the size 68 X 68 X 3. The input feature dimension then becomes 12,288. This will be even bigger if we have larger images (say, of size 720 X 720 X 3). Now, if we feed this big input to a neural network, the number of parameters will swell up to a very large number (depending on the number of hidden layers and hidden units). This will result in more computational and memory requirements – not something most of us can deal with. \par

We begin by looking at edge detection as a simple example. The early layers of a neural network detect edges from an image (Sharma, 2018). Deeper layers might be able to detect the cause of the objects and even more deeper layers might detect the cause of complete objects (like a person’s face). \par

\textbf{Edge Detection Problem}

In this section, we will focus on how the edges can be detected from an image. The examples provided here are adopted from an online tutorial (Sharma, 2018). Suppose we are given the figure below:

\begin{figure}
  \caption{Image with Many Edges}
  \includegraphics[width=150mm, scale=1.1]{edge-fig-1.png}
\end{figure}

As you can see, there are many vertical and horizontal edges in the figure above. Therefore, the first thing to be done is to detect these edges.

\begin{figure}
  \includegraphics[width=120mm]{edge-fig-2.png}
\end{figure}

How do we detect these edges? The first thing to do is assuming the image is BW, we represent it by a pixel map of gray scale values. Assume it is a 6 x 6 matrix.

\begin{figure}
  \includegraphics[width=120mm]{matrix-fig-3.png}
\end{figure}

The values in these cells show pixel values in grayscale. Next, we convolve this 6 X 6 matrix with a 3 X 3 filter. They are also sometime referred to as feature detector or kernel.

\begin{figure}
  \includegraphics[width=120mm]{mat-filter-fig-4.png}
\end{figure}

We take the feature filter and apply it over the original image. By placing it on the left upper corner, we get:

\begin{figure}
  \includegraphics[width=80mm, scale=1.1]{mat-fig-5.png}
\end{figure}

So, we take the first 3 X 3 matrix from the 6 X 6 image and multiply it with the filter. Now, the first element of the output (which is a 4 X 4 matrix) will be the sum of the element-wise product of these values, i.e. 3*1 + 0 + 1*-1 + 1*1 + 5*0 + 8*-1 + 2*1 + 7*0 + 2*-1 = -5. To calculate the second element of the 4 X 4 output, we will shift our filter one step towards the right and again get the sum of the element-wise product:

\begin{figure}
  \includegraphics[width=80mm, scale=1.1]{mat-fig-6.png}
\end{figure}

Similarly, we will convolve over the entire image and get a 4 X 4 output:

\begin{figure}
  \includegraphics[width=80mm, scale=1.1]{mat-fig-7.png}
\end{figure}

The shifting of the filter is also called a stride. A stride of 1 shifts it one cell to the right, while a stride of 2 will shift it 2 cells to the right. So, convolving a 6 X 6 input with a 3 X 3 filter gave us an output of 4 X 4. This output is sometimes referred to as feature map. While the above explains what convolve operation does, we will take another example which can clarify how edges can be detected. If high pixel values represent bright areas and low values represent darker areas of an image, then look at this following example.

\begin{figure}
  \includegraphics[width=130mm]{conv-fig-8.png}
\end{figure}

Higher pixel values represent the brighter portion of the image and the lower pixel values represent the darker portions. Looking at the 4 x 4 output matrix, we can detect a vertical edge in an image.
\par
In mathematics, functional analysis,  convolution is a mathematical operation on two functions (f and g) to produce a third function that expresses how the shape of one is modified by the other. The convolution of f and g is written f∗g, using an asterisk or star. It is defined as the integral of the product of the two functions after one is reversed and shifted. As such, it is a specific kind of integral transform:
$$ [f*g](t) = \int_{0}^{t} f(\uptau) g(t - \uptau) \mathrm{d} \uptau $$


\begin{flushleft}
  \textbf{Kernel Types}
\end{flushleft}

A number of well-known filters or kernels have been used in prior work (Sharma, 2018). Some common ones include Sharpen, Blue, Edge-Detect, or Edge-Enhance.

\includegraphics[width=120mm]{filters-fig-9.png}

The Sobel filter puts a little bit more weight on the central pixels. Instead of using these filters, we can create our own as well and treat them as a parameter which the model will learn using backpropagation. In order to understand the next details of a CNN, let us take a look at the entire series of steps that happens on a single input image.
\par
\includegraphics[width=120mm, scale=0.8]{cnn-fig-10.png}

It is important to note that a single image can generate many feature maps using different kernels. We do this so that different features are extracted out. This first step creates a convolutional layer consisting of many feature maps.
\par
\begin{flushleft}
  \textbf{Padding in Convolution}
\end{flushleft}

We have seen that convolving an input of 6 X 6 dimension with a 3 X 3 filter results in 4 X 4 feature map. We can generalize it and say that if the input is n X n and the filter size is f X f, then the output size will be (n-f+1) X (n-f+1):
\par
\begin{itemize}
  \item Input: n X n
  \item Filter size: f X f
  \item Output: (n-f+1) X (n-f+1)
\end{itemize}

There are two disadvantages to this approach:

\begin{enumerate}
  \item Every time a convolution is applied, the size of the image shrinks.
  \item Pixels present in the corner of the image are used only a few number of times during convolution as compared to the central pixels. Hence, not much focus is put on the corners since that can lead to a loss of information.
\end{enumerate}

In order to overcome these issues, we can pad the image with an additional border, i.e., we add one pixel all around the edges. This means that the input will be an 8 X 8 matrix (instead of a 6 X 6 matrix). Applying convolution of 3 X 3 on it will result in a 6 X 6 matrix which is the original shape of the image.
\par

This is where padding is very useful:

\begin{itemize}
  \item Input: n X n
  \item Padding: p
  \item Filter size: f X f
  \item Output: (n+2p-f+1) X (n+2p-f+1)
\end{itemize}

There are two common choices for padding.

\begin{enumerate}
  \item Valid: No padding at all. If we are using valid padding, the output will be (n-f+1) X (n-f+1)
  \item Same: Here, we apply padding so that the output size is the same as the input size, i.e., n+2p-f+1 = n ;therefore,  p = (f-1)/2
\end{enumerate}

We now know how to use padded convolution. This way we don’t lose a lot of information and the image does not shrink either. It is also important to note that the ReLU function (Rectifier Linear Function) is applied during the kernel application step since images are often very non-linear and we want to ensure that convolution does not create something too linear. Hence applying the ReLU rectifier function helps to solve this problem.

\begin{flushleft}
  \textbf{Pooling or Subsampling}
\end{flushleft}

Pooling layers are generally used to reduce the size of the inputs and hence speed up the computation. Consider a 4 X 4 matrix as shown below:

\includegraphics[width=100mm, scale=1.1]{mat-fig-11.png}

Applying max pooling on this matrix will result in a 2 X 2 output:

\includegraphics[width=100mm, scale=1.1]{max-pool-fig-12.png}

It is called max pooling as for every consecutive 2 X 2 block, we take the maximum number. Here, we have applied a filter of size 2 and a stride of 2. These are the hyperparameters for the pooling layer. Apart from max pooling, we can also apply average pooling where, instead of taking the max of the numbers, we take their average. In summary, the hyperparameters for a pooling layer are:

\begin{enumerate}
  \item Filter size
  \item Stride
  \item Max or average pooling
\end{enumerate}


If the input of the pooling layer is nh X nw X nc, then the output will be:
$$ \frac{(n_h – f)}{s + 1} \times \frac{(n_w – f)}{s + 1} \times n_c $$

In fact what max pooling does is that it takes features from convolutional layer and tries to pool all the relevant features and discard those that doesn’t help us in identification purposes.

\begin{flushleft}
  \textbf{Convolution for Colored Images: A Third Dimension}
\end{flushleft}

So far we have taken a single grayscale image as input represented by n x m matrix in two dimensions. However in real world we often get colored or 3-dimensional images. How will we apply convolution on this image? We will use a 3 X 3 X 3 filter instead of a 3 X 3 filter. Let’s look at an example:

\begin{itemize}
  \item 6 X 6 X 3
  \item 3 X 3 X 3
\end{itemize}

\begin{figure}
  \caption{3D Convolution with a 3D Filter}
  \includegraphics[width=130mm, scale=0.8]{3D-conv-fig-13.png}
\end{figure}

Since there are three channels in the input, the filter will consequently also have three channels. After convolution, the output shape is a 4 X 4 matrix feature map. So, the first element of the output is the sum of the element-wise product of the first 27 values from the input (9 values from each channel) and the 27 values from the filter. After that we convolve over the entire image, seen in Figure 6.
\par
As mentioned earlier, instead of using one kernel, we could use different kernels to extract different features. How do we do that? Let’s say the first filter will detect vertical edges and the second filter will detect horizontal edges from the image. If we use multiple filters, the output dimension will change. So, instead of having a 4 X 4 output as in the above example, we would have a 4 X 4 X 2 output (if we have used 2 filters).
\par
Generalized dimensions can be given as:
\begin{itemize}
  \item Input: n X n X n \textsubscript{c}
  \item Filter: f X f X n \textsubscript{c}
  \item Padding: p
  \item Stride: s
\end{itemize}

The output of this convolution would be:
$$ [\frac{(n+2p-f)}{s+1} \times \frac{(n+2p-f)}{s+1} \times n_{c}^\prime] $$

In this output, $n_c$ is the number of channels in the input and filter, while $ n_c^\prime $ is the number of filters.

In a convolutional neural network, there are basically three types of layers:

\begin{enumerate}
  \item Convolution layer
  \item Pooling layer
  \item Fully connected layer
\end{enumerate}

Once we get an output after convolving over the entire image using a filter, we add a bias term to those outputs and finally apply an activation function to generate activations. This is one layer of a convolutional network . In our case, input image is (6 X 6 X 3) and filters are (3 X 3 X 3). These activations from layer 1 act as the input for layer 2, and so on. Clearly, the number of parameters in case of convolutional neural networks is independent of the size of the image. It essentially depends on the filter size. Suppose we have 10 filters, each of shape 3 X 3 X 3. It is easy to calculate the number of parameters:

\begin{itemize}
  \item Number of parameters for each filter: 3\textsuperscript{3} = 27
  \item There will be a bias term for each filter, so total parameters per filter = 28
  \item Since there are 10 filters, total parameters will be 28*10 = 280
\end{itemize}

No matter how big the image is, the parameters only depend on the filter size. This is a powerful feature of CNNs.
\par
From the convolutional layer, we can do max pooling to create subsampling layer and can again do multiple convolutions and max pooling until we get a reasonable number of output numbers which can be fed to an Artificial Neural Network to learn.
\par
Here is what a full CNN would look like based on some of the examples we have used so far. In Figure 7, we also show notation for filter size, padding if used and the stride values.

\begin{figure}
  \caption{A Complete CNN Example}
  \includegraphics[width=150mm, scale=0.8]{complete-cnn.png}
\end{figure}

There are a combination of convolution and pooling layers at the beginning, a few fully connected layers at the end and finally a softmax classifier to classify the input into various categories. There are a lot of hyperparameters in this network which we have to specify as well.
\par
Generally, we take the set of hyperparameters which have been used in proven research and they end up doing well. As seen in the above example, the height and width of the input shrinks as we go deeper into the network (from 32 X 32 to 5 X 5) and the number of channels increases (from 3 to 10).
\par
There are a number of hyperparameters that we can tweak while building a convolutional network.
These include the number of filters, size of filters, stride to be used, padding, etc. We will look at each of these in detail later in this article. Just keep in mind that as we go deeper into the network, the size of the image shrinks whereas the number of channels usually increases.
